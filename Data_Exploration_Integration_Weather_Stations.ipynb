{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"El_Paso_NOAA_data\\NOAA_El_Paso_1998-2000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to have separate dataframes for every distinct year of NOAA data and write each year to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_stations = set()\n",
    "\n",
    "for row in df.iterrows():\n",
    "#     print(row[1]['STATION'])\n",
    "    temp_tuple = (row[1]['STATION'], row[1]['LATITUDE'], row[1]['LONGITUDE'])\n",
    "    set_of_stations.add(temp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set_of_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"El_Paso_NOAA_data\\NOAA_El_Paso_1998-2019.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df['DATE'].tolist()\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for element in df1:\n",
    "    if element.year == 2001:\n",
    "        count += 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for year in range(1998, 2020):\n",
    "    \n",
    "    temp_list = []\n",
    "    temp_set = set()\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(i)\n",
    "        \n",
    "        if year == row['DATE'].year:\n",
    "            temp_tuple = (row['STATION'], row['LATITUDE'], row['LONGITUDE'])\n",
    "            temp_set.add(temp_tuple)\n",
    "    \n",
    "    my_list = list(temp_set)\n",
    "\n",
    "    with open('weather_stations_{}.txt'.format(year), 'w') as fp:\n",
    "        fp.write('\\n'.join('{},{},{}'.format(x[0],x[1], x[2]) for x in my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = list(temp_set)\n",
    "\n",
    "with open('test.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('{},{},{}'.format(x[0],x[1], x[2]) for x in my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDataList = []\n",
    "\n",
    "with open('El_Paso_Weather_Stations/weather_stations_2019.txt'.format(year), 'r') as f:\n",
    "    for line in f:\n",
    "#         print(line.split(','))\n",
    "        tempDataList.append({'ID': line.split(',')[0], 'lat': line.split(',')[1], 'lon': line.split(',')[2].replace('\\n', '')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the Data_Exploration_Integration files, remove all rows that are not bridges located in El Paso County\n",
    "\n",
    "# Using geolocator to search every coordinate pair to compare the county to see if it belongs to El Paso County or not would take approx. 15 - 18 hours so I approximated the bounding box of El Paso County to compare the numerical coordinates rather than use a geolocator which takes time\n",
    "\n",
    "# I used: https://boundingbox.klokantech.com/ to form an approximate bounding box for El Paso County which returned min latitude: 254.929456, min longitude: 38.519019, max latitude: 255.948379, max longitude: 39.130161\n",
    "\n",
    "# First run is for Data_Exploration_Integration_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for idx in range(1, 10):\n",
    "    NBI_df = pd.read_csv('Data_Exploration_Integration_{}.csv'.format(idx))\n",
    "\n",
    "    NBI_df = NBI_df.dropna(subset=['LAT_016'])\n",
    "    NBI_df = NBI_df.dropna(subset=['LONG_017'])\n",
    "\n",
    "    NBI_df['LAT_016'] = NBI_df['LAT_016'].astype(int)\n",
    "    NBI_df['LONG_017'] = NBI_df['LONG_017'].astype(int)\n",
    "\n",
    "    def dms2dd(s):\n",
    "\n",
    "        minutes = s[-6:-4]\n",
    "        seconds = s[-4:]\n",
    "        seconds_decimal = s[-4:-2] + \".\" +s[-2:]\n",
    "\n",
    "        temp_dms = minutes\n",
    "        temp_dms = temp_dms + seconds\n",
    "\n",
    "        if temp_dms in s:\n",
    "            degrees = s.replace(temp_dms,'')\n",
    "\n",
    "        dd = float(degrees) + float(minutes)/60 + float(seconds_decimal)/(60*60);\n",
    "\n",
    "        return dd\n",
    "\n",
    "    decimal_lats = []\n",
    "    decimal_longs = []\n",
    "\n",
    "    for index, row in NBI_df.iterrows():\n",
    "        str_lat = str(row[\"LAT_016\"])\n",
    "        str_long = str(row[\"LONG_017\"])\n",
    "\n",
    "        if len(str_lat) < 8 or len(str_long) < 8:\n",
    "            NBI_df = NBI_df.drop([index])\n",
    "            continue\n",
    "\n",
    "        decimal_lat = dms2dd(str_lat)\n",
    "        decimal_long = dms2dd(str_long)\n",
    "\n",
    "        decimal_lats.append(decimal_lat)\n",
    "        decimal_longs.append(decimal_long)\n",
    "\n",
    "    decimal_lats\n",
    "    decimal_longs\n",
    "\n",
    "    decimal_longs = [360 - x for x in decimal_longs]\n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    df_new['decimal_lats'] = decimal_lats\n",
    "    df_new['decimal_longs'] = decimal_longs\n",
    "\n",
    "    df_new\n",
    "\n",
    "    NBI_df['LAT_016'] = decimal_lats\n",
    "    NBI_df['LONG_017'] = decimal_longs\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for index, row in NBI_df.iterrows():\n",
    "        count += 1\n",
    "        print(count)\n",
    "\n",
    "        try:\n",
    "            #min latitude: 254.929456, min longitude: 38.519019, max latitude: 255.948379, max longitude: 39.130161\n",
    "            if 38.519019 > row['LAT_016'] or 39.130161 < row['LAT_016'] or 254.929456 > row['LONG_017'] or 255.948379 < row['LONG_017']:\n",
    "                NBI_df = NBI_df.drop([index])\n",
    "        except:\n",
    "            NBI_df = NBI_df.drop([index])\n",
    "            continue\n",
    "\n",
    "    NBI_df.to_csv('Data_Exploration_Integration_El_Paso_{}.csv'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
